{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Language Model Training on Databricks\n",
        "\n",
        "This notebook trains a Transformer language model using **PyTorch DDP** on **multiple GPUs**.\n",
        "\n",
        "## Cluster Requirements\n",
        "\n",
        "- **Runtime**: DBR 13.3 ML or higher (with PyTorch 2.0+)\n",
        "- **Driver**: Instance with multiple GPUs (e.g., `p3.16xlarge` with 8x V100)\n",
        "- **Workers**: 0 (single-node multi-GPU)\n",
        "- **Libraries**: Will be installed in Step 1\n",
        "\n",
        "## GPU Cluster Options\n",
        "\n",
        "### For 8 GPUs:\n",
        "- `p3.16xlarge` - 8x V100 (16GB each)\n",
        "- `p4d.24xlarge` - 8x A100 (40GB each)\n",
        "- `g5.48xlarge` - 8x A10G (24GB each)\n",
        "\n",
        "### For 4 GPUs:\n",
        "- `p3.8xlarge` - 4x V100 (16GB each)\n",
        "- `g5.12xlarge` - 4x A10G (24GB each)\n",
        "- `g4dn.12xlarge` - 4x T4 (16GB each)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: Check GPU Availability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        props = torch.cuda.get_device_properties(i)\n",
        "        print(f\"\\nGPU {i}: {props.name}\")\n",
        "        print(f\"  Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "print(f\"\\nCurrent directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup Project & Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install einops pyyaml tensorboard tqdm --quiet\n",
        "\n",
        "print(\"Packages installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set project directory\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Modify this path based on where you uploaded the project files\n",
        "PROJECT_DIR = \"/dbfs/tmp/transformer-ddp-lm\"  # Change as needed\n",
        "\n",
        "print(f\"Project directory: {PROJECT_DIR}\")\n",
        "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
        "os.chdir(PROJECT_DIR)\n",
        "print(f\"Changed to: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**: The repository is automatically cloned from GitHub in the cell above.\n",
        "\n",
        "If you need to update the code later:\n",
        "```python\n",
        "%sh\n",
        "cd /dbfs/tmp/transformer-ddp-lm\n",
        "git pull\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Prepare Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data directory and prepare toy dataset\n",
        "!mkdir -p data\n",
        "!python data/prepare_dataset.py --output data/toy_dataset.txt --repeat 100\n",
        "\n",
        "# Verify dataset\n",
        "import os\n",
        "if os.path.exists(\"data/toy_dataset.txt\"):\n",
        "    size = os.path.getsize(\"data/toy_dataset.txt\")\n",
        "    print(f\"\\nDataset created: {size:,} bytes\")\n",
        "else:\n",
        "    print(\"\\nDataset creation failed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Train with Multiple GPUs (DDP)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get number of available GPUs\n",
        "import torch\n",
        "num_gpus = torch.cuda.device_count()\n",
        "\n",
        "print(f\"Starting DDP training with {num_gpus} GPUs...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Launch multi-GPU training\n",
        "!python databricks_train.py --num-gpus {num_gpus} --config configs/default_config.yaml\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Training completed!\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Inference & Text Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate text with trained model\n",
        "!python inference.py --checkpoint checkpoints/best_model.pt --prompt \"The Transformer is\" --max-length 300 --temperature 0.8\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
