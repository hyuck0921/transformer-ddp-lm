# Transformer Language Model Configuration

# Model Architecture
model:
  vocab_size: 256  # Character-level (extended ASCII)
  max_seq_len: 512
  dim: 256
  depth: 6
  heads: 8
  dim_head: 32
  mlp_dim: 1024
  dropout: 0.1
  use_rotary_emb: true

# Training
training:
  batch_size: 32  # Per GPU
  num_epochs: 100
  learning_rate: 3.0e-4
  weight_decay: 0.1
  warmup_steps: 500
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
  
  # Mixed Precision
  use_amp: true
  amp_dtype: "bfloat16"  # "float16" or "bfloat16"
  
  # Optimizer
  optimizer: "adamw"
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1.0e-8
  
  # Learning Rate Schedule
  lr_schedule: "cosine"  # "cosine", "linear", or "constant"
  min_lr_ratio: 0.1

# Data
data:
  dataset_path: "data/toy_dataset.txt"
  train_split: 0.9
  val_split: 0.1
  num_workers: 4
  pin_memory: true
  prefetch_factor: 2

# DDP Settings
distributed:
  backend: "nccl"  # "nccl" for GPU, "gloo" for CPU
  find_unused_parameters: false
  gradient_as_bucket_view: true

# Checkpointing
checkpoint:
  save_dir: "checkpoints"
  save_every: 10  # Save every N epochs
  keep_last: 3  # Keep last N checkpoints
  save_optimizer: true

# Logging
logging:
  log_dir: "logs"
  log_every: 10  # Log every N steps
  use_tensorboard: true
  use_wandb: false
  wandb_project: "transformer-lm"
  wandb_entity: null

# Evaluation
evaluation:
  eval_every: 1  # Evaluate every N epochs
  eval_steps: 100  # Number of validation steps
  generate_samples: true
  num_samples: 5
  sample_length: 200

# Generation
generation:
  temperature: 0.8
  top_k: 50
  top_p: 0.9

# Seed
seed: 42

